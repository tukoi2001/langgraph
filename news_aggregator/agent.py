import csv
import json
import os
from datetime import datetime
from typing import Dict, List, Optional
from urllib.parse import urljoin, urlparse
from typing import TYPE_CHECKING

import feedparser # type: ignore
import requests # type: ignore
from bs4 import BeautifulSoup # type: ignore
from dotenv import load_dotenv # type: ignore
from langchain_core.prompts import PromptTemplate # type: ignore
from langchain_openai import ChatOpenAI # type: ignore

load_dotenv(".env")

try:
    from langgraph.graph import StateGraph # type: ignore
    from typing import TypedDict
    
    class NewsAggregatorState(TypedDict):
        """State for the news aggregator graph"""
        topic: str
        rss_urls: list
        website_urls: list
        results: dict
    
    def create_news_aggregator_graph():
        """Create a LangGraph graph for news aggregation"""
        graph = StateGraph(NewsAggregatorState)
        
        def aggregate_news_node(state: NewsAggregatorState):
            """Main node that runs the news aggregator"""
            aggregator = NewsAggregator(topic=state["topic"])
            results = aggregator.run(
                rss_urls=state.get("rss_urls", []),
                website_urls=state.get("website_urls", [])
            )
            return {"results": results}
        
        graph.add_node("aggregate", aggregate_news_node)
        graph.set_entry_point("aggregate")
        graph.set_finish_point("aggregate")
        
        return graph.compile()
    
    app = create_news_aggregator_graph()
except ImportError:
    app = None


class Config:
    CUSTOM_BASE_URL = os.environ.get("CUSTOM_BASE_URL")
    OPENAI_API_KEY = os.environ.get("OPENAI_API_KEY")
    OPENAI_MODEL = "gpt-4o-mini"
    OPENAI_TEMPERATURE = 0
    LANGSMITH_TRACING = os.environ.get("LANGSMITH_TRACING")
    LANGCHAIN_PROJECT = os.environ.get("LANGSMITH_PROJECT")
    LANGCHAIN_API_KEY = os.environ.get("LANGCHAIN_API_KEY", "")
    REQUEST_TIMEOUT = 10
    MAX_ARTICLES_PER_SOURCE = 5
    MAX_CONTENT_LENGTH = 1000
    OUTPUT_DIR = "output"
    
    # Website crawling config
    USER_AGENT = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
    MAX_CRAWL_DEPTH = 2  # Äá»™ sÃ¢u crawl tá»‘i Ä‘a
    MAX_ARTICLES_PER_WEBSITE = 10
    
    @classmethod
    def setup_environment(cls):
        if cls.LANGSMITH_TRACING:
            os.environ["LANGSMITH_TRACING"] = cls.LANGSMITH_TRACING
        if cls.LANGCHAIN_PROJECT:
            os.environ["LANGCHAIN_PROJECT"] = cls.LANGCHAIN_PROJECT
        if cls.LANGCHAIN_API_KEY:
            os.environ["LANGCHAIN_API_KEY"] = cls.LANGCHAIN_API_KEY
        os.makedirs(cls.OUTPUT_DIR, exist_ok=True)
    
    @classmethod
    def validate(cls):
        if not cls.OPENAI_API_KEY:
            raise ValueError("âŒ OPENAI_API_KEY chÆ°a Ä‘Æ°á»£c thiáº¿t láº­p!")
        print("âœ… Cáº¥u hÃ¬nh há»£p lá»‡")


class WebsiteCrawler:
    """Class chuyÃªn crawl tin tá»©c tá»« website"""
    
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': Config.USER_AGENT,
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'vi-VN,vi;q=0.9,en-US;q=0.8,en;q=0.7',
        })
        self.visited_urls = set()
    
    def is_valid_url(self, url: str) -> bool:
        """Kiá»ƒm tra URL há»£p lá»‡"""
        try:
            result = urlparse(url)
            return all([result.scheme, result.netloc])
        except:
            return False
    
    def get_page_content(self, url: str) -> Optional[BeautifulSoup]:
        """Láº¥y ná»™i dung trang web"""
        try:
            response = self.session.get(url, timeout=Config.REQUEST_TIMEOUT)
            response.raise_for_status()
            response.encoding = response.apparent_encoding
            return BeautifulSoup(response.text, 'html.parser')
        except Exception as e:
            print(f"  âš ï¸ Lá»—i táº£i trang: {str(e)}")
            return None
    
    def extract_article_links(self, soup: BeautifulSoup, base_url: str) -> List[str]:
        """TrÃ­ch xuáº¥t link bÃ i viáº¿t tá»« trang"""
        links = []
        
        # TÃ¬m cÃ¡c tháº» a cÃ³ chá»©a link bÃ i viáº¿t
        # ThÆ°á»ng cÃ³ class nhÆ°: article, post, news-item, title, headline
        selectors = [
            'a[href*="/article"]',
            'a[href*="/post"]',
            'a[href*="/news"]',
            'a[href*="/tin-tuc"]',
            'a[href*="/bai-viet"]',
            'article a',
            '.article a',
            '.post a',
            '.news-item a',
            'h2 a',
            'h3 a'
        ]
        
        for selector in selectors:
            for link in soup.select(selector):
                href = link.get('href')
                if href:
                    full_url = urljoin(base_url, href)
                    if self.is_valid_url(full_url) and full_url not in self.visited_urls:
                        links.append(full_url)
        
        return list(set(links))[:Config.MAX_ARTICLES_PER_WEBSITE]
    
    def extract_article_content(self, soup: BeautifulSoup, url: str) -> Optional[Dict]:
        """TrÃ­ch xuáº¥t ná»™i dung bÃ i viáº¿t"""
        try:
            # TÃ¬m tiÃªu Ä‘á»
            title = None
            for selector in ['h1', 'h2.title', '.article-title', '.post-title', 'h1.headline']:
                title_tag = soup.select_one(selector)
                if title_tag:
                    title = title_tag.get_text(strip=True)
                    break
            
            if not title:
                title = soup.title.string if soup.title else "KhÃ´ng cÃ³ tiÃªu Ä‘á»"
            
            # TÃ¬m ná»™i dung chÃ­nh
            content = ""
            content_selectors = [
                'article',
                '.article-content',
                '.post-content',
                '.entry-content',
                '.news-content',
                '.content-detail',
                'div[itemprop="articleBody"]',
                '.detail-content'
            ]
            
            for selector in content_selectors:
                content_tag = soup.select_one(selector)
                if content_tag:
                    # Loáº¡i bá» cÃ¡c tháº» khÃ´ng cáº§n thiáº¿t
                    for tag in content_tag.find_all(['script', 'style', 'iframe', 'nav', 'aside']):
                        tag.decompose()
                    
                    # Láº¥y text tá»« cÃ¡c Ä‘oáº¡n vÄƒn
                    paragraphs = content_tag.find_all(['p', 'div'])
                    content = ' '.join([p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True)])
                    break
            
            # Náº¿u khÃ´ng tÃ¬m tháº¥y content, láº¥y tá»« toÃ n bá»™ body
            if not content:
                body = soup.find('body')
                if body:
                    paragraphs = body.find_all('p')
                    content = ' '.join([p.get_text(strip=True) for p in paragraphs[:5]])
            
            # TÃ¬m ngÃ y Ä‘Äƒng
            published = None
            date_selectors = [
                'time',
                '.published-date',
                '.post-date',
                '.date',
                'meta[property="article:published_time"]',
                'meta[name="pubdate"]'
            ]
            
            for selector in date_selectors:
                date_tag = soup.select_one(selector)
                if date_tag:
                    published = date_tag.get('datetime') or date_tag.get('content') or date_tag.get_text(strip=True)
                    break
            
            # Láº¥y domain lÃ m source
            domain = urlparse(url).netloc
            
            if len(content) < 50:
                return None
            
            return {
                'title': title[:200],
                'link': url,
                'published': published or 'N/A',
                'summary': content[:1000],
                'source': domain,
                'source_type': 'website'
            }
            
        except Exception as e:
            print(f"  âš ï¸ Lá»—i trÃ­ch xuáº¥t: {str(e)}")
            return None
    
    def crawl_website(self, url: str, topic: str = "") -> List[Dict]:
        """Crawl tin tá»©c tá»« má»™t website"""
        articles = []
        
        print(f"\nğŸŒ Äang crawl website: {url}")
        
        # Láº¥y trang chá»§
        soup = self.get_page_content(url)
        if not soup:
            return articles
        
        # TrÃ­ch xuáº¥t cÃ¡c link bÃ i viáº¿t
        article_links = self.extract_article_links(soup, url)
        print(f"  ğŸ“‹ TÃ¬m tháº¥y {len(article_links)} link bÃ i viáº¿t")
        
        # Crawl tá»«ng bÃ i viáº¿t
        for idx, link in enumerate(article_links, 1):
            if link in self.visited_urls:
                continue
            
            self.visited_urls.add(link)
            print(f"  [{idx}/{len(article_links)}] Äang xá»­ lÃ½: {link[:70]}...")
            
            article_soup = self.get_page_content(link)
            if not article_soup:
                continue
            
            article = self.extract_article_content(article_soup, link)
            if article:
                articles.append(article)
                print(f"    âœ… ÄÃ£ láº¥y: {article['title'][:60]}...")
        
        print(f"  âœ… Crawl xong: {len(articles)} bÃ i viáº¿t")
        return articles


class NewsAggregator:
    """Há»‡ thá»‘ng thu tháº­p vÃ  tá»•ng há»£p tin tá»©c tá»± Ä‘á»™ng"""
    
    def __init__(self, topic: str):
        self.topic = topic
        self.llm = ChatOpenAI(
            model=Config.OPENAI_MODEL,
            temperature=Config.OPENAI_TEMPERATURE,
            openai_api_key=Config.OPENAI_API_KEY,
            openai_api_base=Config.CUSTOM_BASE_URL,
        )
        self.crawler = WebsiteCrawler()
        self.articles = []
        print(f"ğŸ¯ Khá»Ÿi táº¡o NewsAggregator cho chá»§ Ä‘á»: '{topic}'")
    
    def test_rss_feed(self, url: str) -> bool:
        """Kiá»ƒm tra xem URL cÃ³ pháº£i RSS feed há»£p lá»‡ khÃ´ng"""
        try:
            print(f"ğŸ” Äang test RSS: {url}")
            feed = feedparser.parse(url)
            if feed.entries:
                print(f"  âœ… RSS há»£p lá»‡! TÃ¬m tháº¥y {len(feed.entries)} bÃ i viáº¿t")
                print(f"  ğŸ“° TiÃªu Ä‘á» feed: {feed.feed.get('title', 'N/A')}")
                if feed.entries:
                    print(f"  ğŸ“ BÃ i Ä‘áº§u tiÃªn: {feed.entries[0].get('title', 'N/A')[:60]}...")
                return True
            else:
                print(f"  âŒ KhÃ´ng pháº£i RSS feed hoáº·c khÃ´ng cÃ³ bÃ i viáº¿t")
                if feed.bozo:
                    print(f"  âš ï¸ Lá»—i parse: {feed.bozo_exception}")
                return False
        except Exception as e:
            print(f"  âŒ Lá»—i: {str(e)}")
            return False
    
    def test_website(self, url: str) -> bool:
        """Kiá»ƒm tra xem website cÃ³ thá»ƒ crawl Ä‘Æ°á»£c khÃ´ng"""
        try:
            print(f"ğŸ” Äang test website: {url}")
            soup = self.crawler.get_page_content(url)
            if soup:
                links = self.crawler.extract_article_links(soup, url)
                print(f"  âœ… Website há»£p lá»‡! TÃ¬m tháº¥y {len(links)} link bÃ i viáº¿t")
                return len(links) > 0
            return False
        except Exception as e:
            print(f"  âŒ Lá»—i: {str(e)}")
            return False
    
    def fetch_rss_feeds(self, rss_urls: List[str]) -> List[Dict]:
        """Thu tháº­p tin tá»©c tá»« cÃ¡c nguá»“n RSS"""
        articles = []
        
        if not rss_urls:
            return articles
        
        print(f"\nğŸ” Äang thu tháº­p tin tá»©c tá»« {len(rss_urls)} nguá»“n RSS...")
        print("-" * 80)
        
        for idx, url in enumerate(rss_urls, 1):
            try:
                print(f"[{idx}/{len(rss_urls)}] Äang xá»­ lÃ½: {url}")
                feed = feedparser.parse(url)
                
                if not feed.entries:
                    print(f"  âš ï¸ KhÃ´ng cÃ³ bÃ i viáº¿t nÃ o")
                    continue
                
                source_name = feed.feed.get("title", url)
                count = 0
                
                for entry in feed.entries[:Config.MAX_ARTICLES_PER_SOURCE]:
                    article = {
                        'title': entry.get('title', 'KhÃ´ng cÃ³ tiÃªu Ä‘á»'),
                        'link': entry.get('link', ''),
                        'published': entry.get('published', 'N/A'),
                        'summary': entry.get('summary', entry.get('description', '')),
                        'source': source_name,
                        'source_type': 'rss'
                    }
                    articles.append(article)
                    count += 1
                
                print(f"  âœ… ÄÃ£ láº¥y {count} bÃ i viáº¿t")
                
            except Exception as e:
                print(f"  âŒ Lá»—i: {str(e)}")
        
        print(f"\nâœ… Tá»•ng sá»‘ bÃ i viáº¿t tá»« RSS: {len(articles)}")
        return articles
    
    def fetch_websites(self, website_urls: List[str]) -> List[Dict]:
        """Thu tháº­p tin tá»©c tá»« cÃ¡c website"""
        articles = []
        
        if not website_urls:
            return articles
        
        print(f"\nğŸŒ Äang crawl tin tá»©c tá»« {len(website_urls)} website...")
        print("-" * 80)
        
        for idx, url in enumerate(website_urls, 1):
            try:
                print(f"[{idx}/{len(website_urls)}] Äang crawl: {url}")
                crawled_articles = self.crawler.crawl_website(url, self.topic)
                articles.extend(crawled_articles)
            except Exception as e:
                print(f"  âŒ Lá»—i: {str(e)}")
        
        print(f"\nâœ… Tá»•ng sá»‘ bÃ i viáº¿t tá»« website: {len(articles)}")
        return articles
    
    def filter_by_topic(self, articles: List[Dict]) -> List[Dict]:
        """Lá»c tin tá»©c theo chá»§ Ä‘á» sá»­ dá»¥ng LangChain"""
        print(f"\nğŸ” Äang lá»c tin tá»©c liÃªn quan Ä‘áº¿n '{self.topic}'...")
        print("-" * 80)
        
        filter_prompt = PromptTemplate(
            input_variables=["topic", "title", "summary"],
            template="""Báº¡n lÃ  má»™t chuyÃªn gia phÃ¢n loáº¡i tin tá»©c.

            PhÃ¢n tÃ­ch xem bÃ i viáº¿t sau cÃ³ TRá»°C TIáº¾P liÃªn quan Ä‘áº¿n chá»§ Ä‘á» "{topic}" khÃ´ng.

            TiÃªu Ä‘á»: {title}
            TÃ³m táº¯t: {summary}

            Chá»‰ tráº£ lá»i "CÃ³ liÃªn quan" náº¿u bÃ i viáº¿t cÃ³ liÃªn quan TRá»°C TIáº¾P vÃ  RÃ• RÃ€NG Ä‘áº¿n chá»§ Ä‘á».
            Tráº£ lá»i "KhÃ´ng liÃªn quan" náº¿u chá»‰ liÃªn quan giÃ¡n tiáº¿p hoáº·c Ä‘á» cáº­p qua loa.

            CÃ¢u tráº£ lá»i (chá»‰ CÃ³ liÃªn quan hoáº·c KhÃ´ng liÃªn quan):"""
        )
        
        chain = filter_prompt | self.llm
        filtered_articles = []
        
        for idx, article in enumerate(articles, 1):
            try:
                result = chain.invoke({
                    "topic": self.topic,
                    "title": article['title'],
                    "summary": article['summary'][:500]
                })
                if "CÃ“ LIÃŠN QUAN" in result.content.upper():
                    filtered_articles.append(article)
                    source_icon = "ğŸ“¡" if article.get('source_type') == 'rss' else "ğŸŒ"
                    print(f"[{idx}/{len(articles)}] âœ… {source_icon} Giá»¯: {article['title'][:60]}...")
                else:
                    print(f"[{idx}/{len(articles)}] âŒ Loáº¡i: {article['title'][:60]}...")
                    
            except Exception as e:
                print(f"[{idx}/{len(articles)}] âš ï¸ Lá»—i: {str(e)}")
        
        print(f"\nâœ… Sá»‘ bÃ i viáº¿t sau khi lá»c: {len(filtered_articles)}/{len(articles)}")
        return filtered_articles
    
    def summarize_articles(self, articles: List[Dict]) -> str:
        """TÃ³m táº¯t vÃ  tá»•ng há»£p cÃ¡c tin tá»©c"""
        print(f"\nğŸ“ Äang tÃ³m táº¯t {len(articles)} bÃ i viáº¿t...")
        print("-" * 80)
        
        # Chuáº©n bá»‹ ná»™i dung
        articles_text = ""
        for i, article in enumerate(articles, 1):
            source_type = "ğŸ“¡ RSS" if article.get('source_type') == 'rss' else "ğŸŒ Website"
            articles_text += f"""
            â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
            BÃ€I VIáº¾T {i} ({source_type})
            â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
            TiÃªu Ä‘á»: {article['title']}
            Nguá»“n: {article['source']}
            NgÃ y: {article.get('published', 'N/A')}
            TÃ³m táº¯t: {article['summary'][:400]}
            """
            
            summary_prompt = PromptTemplate(
                input_variables=["topic", "articles", "count"],
                template="""Báº¡n lÃ  má»™t nhÃ  phÃ¢n tÃ­ch tin tá»©c chuyÃªn nghiá»‡p.

            HÃ£y táº¡o má»™t BÃO CÃO Tá»”NG Há»¢P vá» chá»§ Ä‘á» "{topic}" dá»±a trÃªn {count} bÃ i viáº¿t sau:

            {articles}

            YÃŠU Cáº¦U BÃO CÃO:

            1. Tá»”NG QUAN
            - NÃªu tÃ¬nh hÃ¬nh chung vá» chá»§ Ä‘á»
            - CÃ¡c sá»± kiá»‡n, xu hÆ°á»›ng chÃ­nh Ä‘ang diá»…n ra

            2. PHÃ‚N TÃCH CHI TIáº¾T
            - PhÃ¢n tÃ­ch cÃ¡c khÃ­a cáº¡nh quan trá»ng
            - Dáº«n chá»©ng cá»¥ thá»ƒ tá»« cÃ¡c nguá»“n tin
            - So sÃ¡nh quan Ä‘iá»ƒm khÃ¡c nhau (náº¿u cÃ³)

            3. XU HÆ¯á»šNG VÃ€ Dá»° BÃO
            - CÃ¡c xu hÆ°á»›ng Ä‘Ã¡ng chÃº Ã½
            - TÃ¡c Ä‘á»™ng tiá»m nÄƒng
            - Dá»± bÃ¡o phÃ¡t triá»ƒn

            4. Káº¾T LUáº¬N
            - Tá»•ng káº¿t cÃ¡c Ä‘iá»ƒm chÃ­nh
            - ÄÃ¡nh giÃ¡ tá»•ng quan

            LÆ¯U Ã:
            - Viáº¿t báº±ng tiáº¿ng Viá»‡t, chuyÃªn nghiá»‡p
            - TrÃ­ch dáº«n nguá»“n khi cáº§n thiáº¿t
            - KhÃ¡ch quan, khÃ´ng thiÃªn vá»‹
            - Äá»™ dÃ i: 800-1200 tá»«

            BÃO CÃO:"""
        )
        
        chain = summary_prompt | self.llm
        summary = chain.invoke({
            "topic": self.topic,
            "articles": articles_text,
            "count": len(articles)
        }).content
        
        print("\nâœ… HoÃ n thÃ nh tÃ³m táº¯t")
        return summary
    
    def evaluate_quality(self, summary: str, articles: List[Dict]) -> Dict:
        """ÄÃ¡nh giÃ¡ cháº¥t lÆ°á»£ng tÃ³m táº¯t sá»­ dá»¥ng LangSmith"""
        print("\nâ­ Äang Ä‘Ã¡nh giÃ¡ cháº¥t lÆ°á»£ng bÃ¡o cÃ¡o...")
        print("-" * 80)
        
        eval_prompt = PromptTemplate(
            input_variables=["summary", "num_articles", "word_count"],
            template="""Báº¡n lÃ  má»™t chuyÃªn gia Ä‘Ã¡nh giÃ¡ cháº¥t lÆ°á»£ng bÃ¡o cÃ¡o.

            HÃ£y Ä‘Ã¡nh giÃ¡ bÃ¡o cÃ¡o tá»•ng há»£p tin tá»©c sau:

            â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
            BÃO CÃO Cáº¦N ÄÃNH GIÃ
            â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
            {summary}

            â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
            THÃ”NG TIN
            â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
            - Sá»‘ bÃ i viáº¿t nguá»“n: {num_articles}
            - Sá»‘ tá»«: {word_count}

            ÄÃNH GIÃ theo thang Ä‘iá»ƒm 1-10 cho cÃ¡c tiÃªu chÃ­ sau:

            1. ACCURACY (Äá»™ chÃ­nh xÃ¡c): ThÃ´ng tin cÃ³ chÃ­nh xÃ¡c, khÃ´ng sai lá»‡ch?
            2. COMPLETENESS (Äá»™ Ä‘áº§y Ä‘á»§): CÃ³ bao quÃ¡t cÃ¡c khÃ­a cáº¡nh chÃ­nh khÃ´ng?
            3. CLARITY (Äá»™ rÃµ rÃ ng): Dá»… hiá»ƒu, máº¡ch láº¡c, logic?
            4. OBJECTIVITY (TÃ­nh khÃ¡ch quan): Trung láº­p, khÃ´ng thiÃªn vá»‹?
            5. VALUE (GiÃ¡ trá»‹): Há»¯u Ã­ch, cÃ³ insight hay?

            Tráº£ lá»i ÄÃšNG Ä‘á»‹nh dáº¡ng JSON sau (khÃ´ng thÃªm gÃ¬ khÃ¡c):
            {{
                "accuracy": 8,
                "completeness": 7,
                "clarity": 9,
                "objectivity": 8,
                "value": 7,
                "overall": 7.8,
                "feedback": "BÃ¡o cÃ¡o tá»‘t, rÃµ rÃ ng..."
            }}

            JSON:"""
        )
        
        chain = eval_prompt | self.llm
        
        try:
            word_count = len(summary.split())
            result = chain.invoke({
                "summary": summary[:2000],
                "num_articles": len(articles),
                "word_count": word_count
            }).content
            
            # TrÃ­ch xuáº¥t JSON
            start_idx = result.find('{')
            end_idx = result.rfind('}') + 1
            if start_idx == -1 or end_idx == 0:
                raise ValueError("KhÃ´ng tÃ¬m tháº¥y JSON trong káº¿t quáº£")
            
            json_str = result[start_idx:end_idx]
            evaluation = json.loads(json_str)
            
            # In káº¿t quáº£
            print(f"\nğŸ“Š Káº¾T QUáº¢ ÄÃNH GIÃ:")
            print(f"  â€¢ Äá»™ chÃ­nh xÃ¡c: {evaluation['accuracy']}/10")
            print(f"  â€¢ Äá»™ Ä‘áº§y Ä‘á»§: {evaluation['completeness']}/10")
            print(f"  â€¢ Äá»™ rÃµ rÃ ng: {evaluation['clarity']}/10")
            print(f"  â€¢ TÃ­nh khÃ¡ch quan: {evaluation['objectivity']}/10")
            print(f"  â€¢ GiÃ¡ trá»‹: {evaluation['value']}/10")
            print(f"  â€¢ Tá»”NG THá»‚: {evaluation['overall']}/10")
            print(f"\nğŸ’¬ Nháº­n xÃ©t: {evaluation['feedback']}")
            
            return evaluation
            
        except Exception as e:
            print(f"âš ï¸ KhÃ´ng thá»ƒ Ä‘Ã¡nh giÃ¡: {str(e)}")
            return {
                "error": str(e),
                "overall": 0
            }
    
    @staticmethod
    def _auto_download_file(filepath: str, filename: str, mimetype: str):
        """
        Tá»± Ä‘á»™ng táº£i file vá» mÃ¡y (auto-download)
        Há»— trá»£ cáº£ Jupyter Notebook vÃ  terminal
        """
        import base64
        
        try:
            try:
                from IPython.display import HTML, display, Javascript  # type: ignore
                in_jupyter = True
            except ImportError:
                in_jupyter = False
            
            if in_jupyter:
                # Äá»c file
                with open(filepath, 'rb') as f:
                    file_content = f.read()
                
                b64 = base64.b64encode(file_content).decode()
                
                # Táº¡o JavaScript Ä‘á»ƒ tá»± Ä‘á»™ng táº£i file xuá»‘ng
                js_download = f"""
                var link = document.createElement('a');
                link.href = 'data:{mimetype};base64,{b64}';
                link.download = '{filename}';
                document.body.appendChild(link);
                link.click();
                document.body.removeChild(link);
                """
                
                # Thá»±c thi JavaScript Ä‘á»ƒ táº£i file
                display(Javascript(js_download))
                
                # Hiá»ƒn thá»‹ thÃ´ng bÃ¡o vÃ  link backup
                download_link = f'''
                <div style="padding: 10px; background-color: #e8f5e9; border-left: 4px solid #4caf50; border-radius: 5px; margin: 10px 0;">
                    <p style="margin: 0; color: #2e7d32; font-weight: bold;">âœ… File Ä‘ang Ä‘Æ°á»£c táº£i xuá»‘ng tá»± Ä‘á»™ng...</p>
                    <p style="margin: 5px 0 0 0; font-size: 0.9em;">
                        Náº¿u khÃ´ng tá»± Ä‘á»™ng táº£i, 
                        <a href="data:{mimetype};base64,{b64}" 
                        download="{filename}"
                        style="color: #1976d2; text-decoration: underline;">
                            click vÃ o Ä‘Ã¢y
                        </a>
                    </p>
                </div>
                '''
                display(HTML(download_link))
                print(f"ğŸ“¥ Äang táº£i xuá»‘ng: {filename}")
                
            else:
                # Terminal: Má»Ÿ file explorer/finder táº¡i vá»‹ trÃ­ file
                abs_path = os.path.abspath(filepath)
                print(f"ğŸ“ File Ä‘Ã£ Ä‘Æ°á»£c lÆ°u táº¡i: {abs_path}")
                
                # Thá»­ má»Ÿ file explorer (tÃ¹y há»‡ Ä‘iá»u hÃ nh)
                try:
                    import platform
                    import subprocess
                    
                    system = platform.system()
                    if system == 'Windows':
                        subprocess.Popen(['explorer', '/select,', abs_path])
                        print("ğŸ“‚ ÄÃ£ má»Ÿ File Explorer")
                    elif system == 'Darwin':  # macOS
                        subprocess.Popen(['open', '-R', abs_path])
                        print("ğŸ“‚ ÄÃ£ má»Ÿ Finder")
                    elif system == 'Linux':
                        # Thá»­ má»Ÿ file manager
                        subprocess.Popen(['xdg-open', os.path.dirname(abs_path)])
                        print("ğŸ“‚ ÄÃ£ má»Ÿ File Manager")
                except Exception as e:
                    print(f"âš ï¸ KhÃ´ng thá»ƒ má»Ÿ file explorer: {e}")
                
        except Exception as e:
            print(f"âš ï¸ KhÃ´ng thá»ƒ tá»± Ä‘á»™ng táº£i: {e}")
            print(f"ğŸ“ File Ä‘Ã£ Ä‘Æ°á»£c lÆ°u táº¡i: {os.path.abspath(filepath)}")

    def export_to_txt(self, summary: str, articles: List[Dict], evaluation: Dict, filename: str):
        """Xuáº¥t bÃ¡o cÃ¡o ra file TXT vÃ  tá»± Ä‘á»™ng táº£i vá»"""
        filepath = os.path.join(Config.OUTPUT_DIR, filename)
        
        # Táº¡o thÆ° má»¥c náº¿u chÆ°a tá»“n táº¡i
        os.makedirs(Config.OUTPUT_DIR, exist_ok=True)
        
        with open(filepath, 'w', encoding='utf-8') as f:
            # Header
            f.write("â•”" + "â•" * 78 + "â•—\n")
            f.write(f"â•‘{'BÃO CÃO Tá»”NG Há»¢P TIN Tá»¨C'.center(78)}â•‘\n")
            f.write(f"â•‘{self.topic.upper().center(78)}â•‘\n")
            f.write("â•š" + "â•" * 78 + "â•\n\n")
            
            # ThÃ´ng tin
            f.write(f"ğŸ“… NgÃ y táº¡o: {datetime.now().strftime('%d/%m/%Y %H:%M:%S')}\n")
            f.write(f"ğŸ“Š Sá»‘ bÃ i viáº¿t: {len(articles)}\n")
            
            # Thá»‘ng kÃª nguá»“n
            rss_count = sum(1 for a in articles if a.get('source_type') == 'rss')
            web_count = sum(1 for a in articles if a.get('source_type') == 'website')
            f.write(f"ğŸ“¡ Tá»« RSS: {rss_count} bÃ i\n")
            f.write(f"ğŸŒ Tá»« Website: {web_count} bÃ i\n")
            f.write(f"â­ Äiá»ƒm Ä‘Ã¡nh giÃ¡: {evaluation.get('overall', 'N/A')}/10\n")
            f.write("\n" + "â”" * 80 + "\n\n")
            
            # TÃ³m táº¯t
            f.write("ğŸ“ TÃ“M Táº®T Tá»”NG Há»¢P\n")
            f.write("â”" * 80 + "\n\n")
            f.write(summary)
            f.write("\n\n" + "â”" * 80 + "\n\n")
            
            # ÄÃ¡nh giÃ¡
            if 'error' not in evaluation:
                f.write("â­ ÄÃNH GIÃ CHáº¤T LÆ¯á»¢NG\n")
                f.write("â”" * 80 + "\n\n")
                f.write(f"â€¢ Äá»™ chÃ­nh xÃ¡c: {evaluation['accuracy']}/10\n")
                f.write(f"â€¢ Äá»™ Ä‘áº§y Ä‘á»§: {evaluation['completeness']}/10\n")
                f.write(f"â€¢ Äá»™ rÃµ rÃ ng: {evaluation['clarity']}/10\n")
                f.write(f"â€¢ TÃ­nh khÃ¡ch quan: {evaluation['objectivity']}/10\n")
                f.write(f"â€¢ GiÃ¡ trá»‹: {evaluation['value']}/10\n")
                f.write(f"â€¢ Tá»”NG THá»‚: {evaluation['overall']}/10\n\n")
                f.write(f"ğŸ’¬ Nháº­n xÃ©t: {evaluation['feedback']}\n")
                f.write("\n" + "â”" * 80 + "\n\n")
            
            # Danh sÃ¡ch bÃ i viáº¿t
            f.write("ğŸ“š DANH SÃCH BÃ€I VIáº¾T NGUá»’N\n")
            f.write("â”" * 80 + "\n\n")
            
            for i, article in enumerate(articles, 1):
                source_icon = "ğŸ“¡" if article.get('source_type') == 'rss' else "ğŸŒ"
                f.write(f"{i}. {article['title']}\n")
                f.write(f"   {source_icon} Nguá»“n: {article['source']} ({article.get('source_type', 'unknown').upper()})\n")
                f.write(f"   ğŸ”— Link: {article['link']}\n")
                f.write(f"   ğŸ“… NgÃ y: {article.get('published', 'N/A')}\n")
                summary_text = article['summary'][:200] + "..." if len(article['summary']) > 200 else article['summary']
                f.write(f"   ğŸ“„ TÃ³m táº¯t: {summary_text}\n\n")
        
        print(f"âœ… ÄÃ£ xuáº¥t bÃ¡o cÃ¡o TXT: {filepath}")
        
        # Tá»± Ä‘á»™ng táº£i file vá» (cho Jupyter Notebook)
        self._auto_download_file(filepath, filename, 'text/plain')
        
        return filepath

    def export_to_csv(self, articles: List[Dict], filename: str):
        """Xuáº¥t danh sÃ¡ch bÃ i viáº¿t ra file CSV vÃ  tá»± Ä‘á»™ng táº£i vá»"""
        filepath = os.path.join(Config.OUTPUT_DIR, filename)
        
        # Táº¡o thÆ° má»¥c náº¿u chÆ°a tá»“n táº¡i
        os.makedirs(Config.OUTPUT_DIR, exist_ok=True)
        
        with open(filepath, 'w', newline='', encoding='utf-8-sig') as f:
            fieldnames = ['STT', 'TiÃªu Ä‘á»', 'Nguá»“n', 'Loáº¡i nguá»“n', 'Link', 'NgÃ y Ä‘Äƒng', 'TÃ³m táº¯t']
            writer = csv.DictWriter(f, fieldnames=fieldnames)
            
            writer.writeheader()
            
            for i, article in enumerate(articles, 1):
                writer.writerow({
                    'STT': i,
                    'TiÃªu Ä‘á»': article['title'],
                    'Nguá»“n': article['source'],
                    'Loáº¡i nguá»“n': article.get('source_type', 'unknown').upper(),
                    'Link': article['link'],
                    'NgÃ y Ä‘Äƒng': article.get('published', 'N/A'),
                    'TÃ³m táº¯t': article['summary'][:300] + '...' if len(article['summary']) > 300 else article['summary']
                })
        
        print(f"âœ… ÄÃ£ xuáº¥t danh sÃ¡ch CSV: {filepath}")
        
        # Tá»± Ä‘á»™ng táº£i file vá» (cho Jupyter Notebook)
        self._auto_download_file(filepath, filename, 'text/csv')
        
        return filepath
    
    
    
    def run(self, rss_urls: Optional[List[str]] = None, website_urls: Optional[List[str]] = None) -> Dict:
        """
        Cháº¡y toÃ n bá»™ quy trÃ¬nh
        
        Args:
            rss_urls: Danh sÃ¡ch URL RSS feed
            website_urls: Danh sÃ¡ch URL website cáº§n crawl
        
        Returns:
            Dictionary chá»©a káº¿t quáº£
        """
        print("\n" + "=" * 80)
        print("ğŸš€ Báº®T Äáº¦U THU THáº¬P VÃ€ Tá»”NG Há»¢P TIN Tá»¨C")
        print("=" * 80)
        
        all_articles = []
        
        # 1. Thu tháº­p tá»« RSS
        if rss_urls:
            rss_articles = self.fetch_rss_feeds(rss_urls)
            all_articles.extend(rss_articles)
        
        # 2. Thu tháº­p tá»« Website
        if website_urls:
            web_articles = self.fetch_websites(website_urls)
            all_articles.extend(web_articles)
        
        if not all_articles:
            print("\nâŒ KhÃ´ng thu tháº­p Ä‘Æ°á»£c bÃ i viáº¿t nÃ o!")
            return {}
        
        # 3. Lá»c theo chá»§ Ä‘á»
        filtered_articles = self.filter_by_topic(all_articles)
        
        if not filtered_articles:
            print("\nâŒ KhÃ´ng cÃ³ bÃ i viáº¿t nÃ o phÃ¹ há»£p vá»›i chá»§ Ä‘á»!")
            return {}
        
        # 4. TÃ³m táº¯t
        summary = self.summarize_articles(filtered_articles)
        
        # 5. ÄÃ¡nh giÃ¡
        evaluation = self.evaluate_quality(summary, filtered_articles)
        
        # 6. Xuáº¥t file
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        topic_slug = self.topic.replace(" ", "_")
        txt_file = f"bao_cao_{topic_slug}_{timestamp}.txt"
        csv_file = f"danh_sach_{topic_slug}_{timestamp}.csv"
        
        self.export_to_txt(summary, filtered_articles, evaluation, txt_file)
        self.export_to_csv(filtered_articles, csv_file)
        
        # Káº¿t quáº£
        print("\n" + "=" * 80)
        print("âœ… HOÃ€N THÃ€NH!")
        print("=" * 80)
        print(f"\nğŸ“Š THá»NG KÃŠ:")
        print(f"  â€¢ Tá»•ng bÃ i viáº¿t thu tháº­p: {len(all_articles)}")
        
        if rss_urls:
            rss_count = sum(1 for a in all_articles if a.get('source_type') == 'rss')
            print(f"    - Tá»« RSS: {rss_count}")
        
        if website_urls:
            web_count = sum(1 for a in all_articles if a.get('source_type') == 'website')
            print(f"    - Tá»« Website: {web_count}")
        
        print(f"  â€¢ BÃ i viáº¿t sau lá»c: {len(filtered_articles)}")
        print(f"  â€¢ Äá»™ dÃ i tÃ³m táº¯t: {len(summary)} kÃ½ tá»±")
        print(f"  â€¢ Äiá»ƒm Ä‘Ã¡nh giÃ¡: {evaluation.get('overall', 'N/A')}/10")
        print(f"\nğŸ“ FILE OUTPUT:")
        print(f"  â€¢ {txt_file}")
        print(f"  â€¢ {csv_file}")
        print("\n" + "=" * 80 + "\n")
        
        return {
            'summary': summary,
            'articles': filtered_articles,
            'evaluation': evaluation,
            'stats': {
                'total': len(all_articles),
                'rss': sum(1 for a in all_articles if a.get('source_type') == 'rss'),
                'website': sum(1 for a in all_articles if a.get('source_type') == 'website'),
                'filtered': len(filtered_articles)
            },
            'files': {
                'txt': txt_file,
                'csv': csv_file
            }
        }


